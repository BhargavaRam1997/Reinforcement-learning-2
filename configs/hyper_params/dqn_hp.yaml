# DQN hyperparameters as reported in: (with slight modifications)
# Nature DQN paper (https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning)
# OpenAI Baselines

# Hyperparams for replay buffer
hyper_params:
  batch_size: 32
  replay_buffer_size: 100000 # Nature DQN puts 1000000 but this takes up too much memory
  use_per: True
  per_beta: 0.6  # PER beta value
  per_alpha: 0.4  # PER alpha value

  # Exploration configs
  eps: 1.0  # epsilon-greedy exploration
  eps_final: 0.1  # minimum epsilon value for exploration
  max_exploration_frame: 1000000  # eps = eps_final at most until # steps

  # Others
  gamma: 0.99
  learning_rate: 0.00025
  update_starting_point: 5000  # update steps when buffer has # experiences stored
